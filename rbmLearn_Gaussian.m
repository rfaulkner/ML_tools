% rbmLearn_Gaussian.m
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               %
% Ryan Faulkner - 260310308     %    
%                               %
% MSc Thesis                    %
%                               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DESCRIPTION:
%
% Given sample data and parameters learns a binary-gaussian RBM
%
% The conditional probabilities on the binary hidden units remain unchanged
% The conditional probabilities on the real-valued visible units are
% slightly different

function [modelWeights modelBiases indices] = rbmLearn_Gaussian(data, parameters, fid)


% EXTRACT PARAMETERS
% ------------------
learnRate = parameters{1};
momentum = parameters{2};
maxEpoch = parameters{3};
numGibbs = parameters{4};
numBatches = parameters{5};
numHid = parameters{6};
model = parameters{7};

numVis = size(data,2);
numCases = size(data,1);

indices = cell(4,1);
indices{1} = 1 : numVis;                                    % indices for bits of input (visible) vector at t
indices{2} = numVis + 1 : numVis + numHid;                  % indices for bits of input (visible) vector at t+1 

VH_Update = 0;
VBias_Update = 0;
HBias_Update = 0;

count = 0;

%%%%%%%%%%%%%%%%%%
% INITIALIZE MODEL
%%%%%%%%%%%%%%%%%%

modelWeights = cell(1,1);
modelBiases = cell(2,1);

alpha = .01;

if size(model,1) == 0
    modelWeights{1} = alpha * rand(numVis, numHid);

    modelBiases{1} = alpha * rand(1, numVis);
    modelBiases{2} = alpha * rand(1, numHid);
    
    model = cell(2,1);    
else
    modelWeights{1} = model{1}{1};

    modelBiases{1} = model{2}{1};
    modelBiases{2} = model{2}{2}; 
end

samples = zeros(numCases, numVis + numHid);

samples(:,indices{1}) = data(:,indices{1});
samples(:,indices{2}) = rand(numCases,numHid) > 0.5;

[batchdata batchSize] = batchify(samples, numBatches);

initTime = clock;

%%%%%%%%%%%%%% END INITIALIZE


samplesConditional = zeros(batchSize, numVis + numHid);
samplesJoint =  zeros(batchSize, numVis + numHid);

fprintf(fid,'\nRBM, visibles, %d; hiddens, %d; learning rate, %4.6f; momentum, %4.3f; gibbs Iterations, %d; batches, %d\n', ... 
    numVis, numHid, learnRate, momentum, numGibbs, numBatches);

for epoch = 1:maxEpoch


    %%%%%%%%%%%%%%%%%%%%%%%
    % Compute ERROR on selected epochs
    %%%%%%%%%%%%%%%%%%%%%%%
    
    if epoch == 2^count || epoch == maxEpoch
        
         % set params for model 
         visToHid = modelWeights{1};
         hidBias = repmat(modelBiases{2},numCases,1);
         visBias = repmat(modelBiases{1},numCases,1);

         vis = data(:,indices{1});
         
         % COMPUTE CONDITIONAL PROBABILITIES
         % ================================
         
         % SAMPLE BINARY HIDDEN UNITS
         probs = 1./(1 + exp(- vis * visToHid - hidBias));
         hid = probs > rand(numCases, numHid);

         % SAMPLE GAUSSIAN HIDDEN UNITS
         % sample from a gaussian random variable 
         % with mean = vis_bias + hidden_activation
         vis = randn(numCases,numVis) + visBias + hid * visToHid';       
                           
         
         % get the current time
         currTime = clock;
         if currTime(3) > initTime(3)
            relativeTime = currTime(4)*60 + currTime(5) + (23 - initTime(4))*60 + (60 - initTime(5)); 
         else
             relativeTime = (currTime(4) - initTime(4))*60 + (currTime(5) - initTime(5)); 
         end
         
         % COMPUTE the Bitwise Error
         % =========================
         % [bitwiseError numBad] = computeBitwiseError(stateLabels, classifier, data, vis);
         bitwiseError = sum(sum(abs(data - vis))) / (numCases * numVis);
         
         % COMPUTE KL Divergence
         % =====================
         % klVector = computeKLDistance(distribution, q);
         
         % LOG the Learning Analytics
         % ==========================
         fprintf(fid,'EPOCH: %d,\tLEARNING RATE: %5.4f\tERROR: %5.6f,\tTIME: %d mins\n', epoch, learnRate, bitwiseError, relativeTime);
         
                           
         % DISPLAY Model Output
         % =====================
         % if meaningful (ex. MNIST)
         close();
         
         if size(vis,1) >= 100
            displayCases(vis(1:100,:),false);
         else
            displayCases(vis,false);
         end
         
                   
         % GENERATE Plot of MNIST Generated Samples From the Model
         % =======================================================
         
%         title(sprintf('First 100 Samples generated by the model after %d Training epochs', epoch));

%          if epoch == maxEpoch             
%              outputFilename = strcat('outputSamples_',num2str(currTime(2)),'_',num2str(currTime(3)),'_',num2str(currTime(4)),'_',num2str(currTime(5)));
%              print('-depsc', outputFilename);
%          end
          
         % SAVE MODEL
         % ==========
         save('rbmBinGaussSnap.mat');
         
         count = count + 1;         
         
    end
     
    
    %%%%%%%%%%%%
    % LEARNING
    %%%%%%%%%%%%
    
    for batch = 1:numBatches
                
        dataBatch = batchdata{batch}; 
        numBatchCases = size(dataBatch,1);
        vis = dataBatch(:,indices{1});
        
        %%%%%%%%%%%%%%%%
        % GIBBS SAMPLING
        %%%%%%%%%%%%%%%%%
                       
        visToHid = modelWeights{1};
        hidBias = repmat(modelBiases{2},batchSize,1);
        visBias = repmat(modelBiases{1},batchSize,1);
        
        % compute hiddens
        probs = 1./(1 + exp(- vis * visToHid - hidBias));
        hid = probs > rand(batchSize, numHid);
                
        samplesConditional(:,indices{1}) = vis;
        samplesConditional(:,indices{2}) = hid;
        
        for it = 1:numGibbs
            % compute visibles
            vis = randn(numBatchCases,numVis) + visBias + hid * visToHid';
            % probs = 1./(1 + exp(- hid * visToHid' - visBias));
            % vis = probs > rand(batchSize, numVis);

            % compute hiddens
            probs = 1./(1 + exp(- vis * visToHid - hidBias));
            hid = probs > rand(batchSize, numHid);
        end

        samplesJoint(:,indices{1}) = vis;
        samplesJoint(:,indices{2}) = hid;

        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% END GIBBS
            
        
        %%%%%%%%%%%%%%
        % UPDATE MODEL
        %%%%%%%%%%%%%%%             
        
        
        prodConditionalVH = samplesConditional(:,indices{1})' * samplesConditional(:,indices{2}) / batchSize;
        prodConditionalVBias = sum(samplesConditional(:,indices{1})) / batchSize;
        prodConditionalHBias = sum(samplesConditional(:,indices{2})) / batchSize;

        prodJointVH = samplesJoint(:,indices{1})' * samplesJoint(:,indices{2}) / batchSize;
        prodJointVBias = sum(samplesJoint(:,indices{1})) / batchSize;
        prodJointHBias = sum(samplesJoint(:,indices{2})) / batchSize;

        VH_Update = momentum * VH_Update + (prodConditionalVH - prodJointVH) * learnRate;
        VBias_Update = momentum * VBias_Update + (prodConditionalVBias - prodJointVBias) * learnRate;
        HBias_Update = momentum * HBias_Update + (prodConditionalHBias - prodJointHBias) * learnRate;

        modelWeights{1} = modelWeights{1} + VH_Update;
        modelBiases{1} = modelBiases{1} + VBias_Update;
        modelBiases{2} = modelBiases{2} + HBias_Update;
               
        %%%% END MODEL UPDATE
        
    end
    
end


% G_old = ones(numVis, numHid);
% G = ones(numVis, numHid);
% localUpdateGains = G_old .* G > 0;
% 
% G_old = G;
% G = G.*(~localUpdateGains * 0.95 + localUpdateGains + 0.5 * localUpdateGains);


